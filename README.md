# Project Background
This a batch driven ELT pipeline adapted from a real-life client project for restaurant franchise company.

The motivation for this pipeline originated with the company's need for processing large quantities of sales information, at scale, in support of their initiatives to open up dozens of new locations throughout their target region. 

The goal was to have access to continuous and accurate metrics on daily restaurants sales in order to inform short and long term business strategy.

The pipeline uses a batch processing architecture with daily scheduled updates.  Due to client restrictions on available technology, it is designed to run exclusively on AWS.

Feel free to use this project as is or adapt it to your needs. Use it freely for personal or commercial applications.

# Required Metrics
The company defined the following key metrics, listed in priority order, the pipeline needs to support in order to inform key business decisions.

| Metric  | Purpose | Details |
| ------- | ------- | ------- |
| Customer Lifetime Value (CLV) | In order to enable effective prioritization of high-value customers, efficiently plan marketing budgets, and improve retention strategies, the total revenue generated by a given a customer, over the entire lifetime of their relationship with the business, needs to be estimated | For the purposes of analysis, CLV is categorized as follows: **High CLV: Top 20% customers**, **Medium CLV: Mid 60%**, **Low CLV: Bottom 20%** |
| Customer Segmentation & Behavior|In support effective targeting of marketing campaigns, information grouping customers based on spending and activity is needed to enable personalized offers and engagement approaches.| For the purposes of analysis, Customer Segmentation & Behavior is categorized as follows, based on Recency, Frequency, and Monetary (RFM) logic: **VIPs: High R, F, M**, **New Customers: Low F, high R**, **Churn Risk: Low R, low F**, ***Ask what corresponds to high/low F,R?*** |
|  Churn Indicators (non-predictive) | To enable timely retention actions to be taken, a customer activity profile is needed to help marketing identify at-risk customers. | The customer activity profile is defined as follows: **Days since last order**, **Average gap between orders**, **% change in spend over last N periods**, Customers who have not made a purchase in last 45 days are classified “at risk”, ***Assuming this is the average time gap in days?*** ***What is the period length?*** |
|Sales Trends Monitoring|Expose time-based summaries, to enable the analysis of sales patterns, with the goal of identifying peak periods and resource planning.|Time based summaries are to be aggregated, in terms of revenue, for daily, weekly, and monthly intervals. Each summary should be broken down by: Location, Menu category (if available), Time of day (optional) |
|Loyalty Program Impact|Compare loyalty members vs non-members in terms of spend and engagement in order to evaluate the ROI of the loyalty program.|Customers to be compared on by: Average Spend, Repeat Orders, Lifetime Value|
|Top-Performing Locations|Identify best and worst-performing store locations to inform decisions on promotions, staffing, or expansion.|Locations to be compared by: Total revenue, Average order value, Orders per day/week. Locations will be ranked on total revenue |
|Pricing & Discount Effectiveness|Measure how discounts affect revenue and profitability in order optimize pricing strategies.|Orders to be compared by: Revenue from discounted orders vs non-discounted, Number of orders before/after applying discounts |

# Data Model

The data model is broken down into raw, transformed, and mart layers.

The details can be viewed [here](https://drive.google.com/file/d/13dutWa3VpALO5iYgiQx1cYxy8sDyPpxs/view?usp=drive_link).

# Pipeline Architecture

The design of pipeline architecture can be viewed [here](https://drive.google.com/file/d/1-0mgAff__QrRqfGoUGC011BQr_tEw5pc/view?usp=drive_link).

# Considerations and Rationale

In addition to system to technical design constraints, the client specifically limited implementation options to AWS.

The following documents the decisions and considerations taken into account when creating the system design:

| Requirement | Motivation | Technical Rationale |
| ----------- | ---------- | --------- |
| Accessible | Order information arrives from dozens of different locations.  The information needs to be communicable through a medium that is universally accessible and trivial to use.  Operators at different restaurant locations should be able to submit daily order information with minimal effort. The delivery mechanism should also be built for automation. | A combination of **Google Drive** and **S3** were chosen to faciliate data injestion/extraction. **Google Drive** is a universally accessible file service available to anyone with a gmail account.  One or more shared folders can be created where restaurant operators can easily add their data.  Alternatively, order data can also be collected automatically and written to one or more shared folders through simple software tools.  **S3** serves as an ideal entry point for daily order data.  It's storage capacity is also arbitrarily scaleable allowing it to adapt to large increaases in data volume.  Furthermore, it's flexible integration options with other AWS services allow for many options with regards to loading and transformation |
| Scaleable loading | Over the course of the pipeline's operation, the amount of data processed will rise significantly. Though individual files won't amount to more then dozens of MB at a time, the total amount data processed will certainly reach several TB levels  The pipeline should be capable of accommodating an ever-expanding data set as new restaurants are opened and start transmitting their daily data. | Data loading will be handled by **Lambda** functions. Prior to injesting the file into the transformation stage, **Lambda** functions, running inside an **ECR** container, will handle all cleaning and error processing. **Lambda** can support up to 10GB per call which is well within the expected limits of any individual file.  In addition **Lambda** functions can scale to an arbitrary number of instances.
| Scaleable, efficient, and flexible data transformation | Over the course of the pipeline's operation, the size and variety of data is expected to grow and change.  Adapting to changes should be be focused and scaleable both in terms of system design and developer effort. | Tranformation jobs, running inside **ECR** instances, will deployed to an **Apache Spark** instance running on **AWS EMR**. **Apache Spark** accomodates fast and efficient computation of large data sets at scale.  Jobs will be broken down into layers to enable efficient transformation from raw to refined data, and ultimately curated data to be used by the business.
| Scaleable and cost-effective data storage | Data storage and querying requirements will grow predictably but with varying consistency (i.e. not all new restaurants will begin operation at the same time, new restaurants may begin operations at different times in the future) | A serverless configuration of **AWS Redshift** will be used in order to accommodate flexible data storage requirements at scale.  
| Automatable and schedulable | The restaraunt chain operates dozens of restaurants all over it's target region.  Relevant information on operations need to be available a daily basis enabling management to adapt in a timely fashion. Manual operation at scale is slow and error prone.  The pipeline has to operate and respond to ever changing data inputs with minimal to no manual intervention | **Airflow** was chosen in order to coordinate the operation of each of each pipeline stage.  Airflow's flexibility in adding stages, scheduling, and reporting make it a sutiable choice for fulfilling scheduling and automation requirements |
| Information security | Operational data is a business critical resource and must be adequately protected from unauthorized parties | Access to **S3** data from unauthorized parties can be controlled via encryption and access to the bucket can be narrowly restricted using access keys.|

**A note on ECR as the deployment mechanism for all jobs** 
Container based deployments enable the development team to, efficiently and securely, enhance and maintain the data pipeline through its lifecycle.  Using a container mechanism has the following clear advantages:
- Avoids giving the development team direct access to AWS resources which can threaten the stability and security of the platform
- Enables the straightforward integration with CI/CD processes
- Enables the development team to work safely and efficiently and independently on changes to pipeline logic
- Enables for more complex deployments of pipeline logic
- Enables for flexible tooling (i.e. unit testing, code linting, pre-processing, etc) in the development of the pipeline logic